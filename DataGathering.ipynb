{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /anaconda3/lib/python3.7/site-packages (4.2.5)\n"
     ]
    }
   ],
   "source": [
    "from requests import get # import our library\n",
    "from bs4 import BeautifulSoup #import beautiful soup \n",
    "import pandas as pd #import pandas\n",
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data from BoxOfficeMojo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a list of URLS to iterate through and scrape\n",
    "URLS = [\n",
    "'https://www.boxofficemojo.com/alltime/world/?pagenum=1&p=.htm',\n",
    "'https://www.boxofficemojo.com/alltime/world/?pagenum=2&p=.htm',\n",
    "'https://www.boxofficemojo.com/alltime/world/?pagenum=3&p=.htm',\n",
    "'https://www.boxofficemojo.com/alltime/world/?pagenum=4&p=.htm',\n",
    "'https://www.boxofficemojo.com/alltime/world/?pagenum=5&p=.htm',\n",
    "'https://www.boxofficemojo.com/alltime/world/?pagenum=6&p=.htm',\n",
    "'https://www.boxofficemojo.com/alltime/world/?pagenum=7&p=.htm',\n",
    "'https://www.boxofficemojo.com/alltime/world/?pagenum=8&p=.htm'\n",
    "]\n",
    "#Create an empty list to hold what df made from each URL page\n",
    "scraped_dfs = []\n",
    "\n",
    "for url in URLS:\n",
    "    response = get(url) #pull information in the dataframe\n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser') #parse text\n",
    "    main_table = html_soup.find_all('table')#grabs all table objects found on page\n",
    "    table = str(main_table[2]) #select the table we actually want happened to be the 3rd table object on tha pge\n",
    "    dfs = pd.read_html(table)#createa df from everthing in table\n",
    "    df = dfs[0]#assign series as final df from page \n",
    "    df = df.drop([0]) #dropped column titles that happen on every page\n",
    "    df = df.drop([0], axis = 1) #drop rank becaue redundant with index\n",
    "    scraped_dfs.append(df) #add df to a list to concat after the loop\n",
    "\n",
    "BoxOfficedf = pd.concat(scraped_dfs)#concat all the df in the scraped_dfs list\n",
    "#recover the column names\n",
    "BoxOfficedf.columns = ['Title','Studio','Worldwide','Domestic','Dom%','Overseas','Over%', 'Year']\n",
    "#save original data to csv file\n",
    "# BoxOfficedf.to_csv('BoxOffice_DF_original.csv') #commented out becuause already created\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data from Numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a list of URLS to iterate through and scrape\n",
    "URLS = [\n",
    "'https://www.the-numbers.com/movie/budgets/all',\n",
    "'https://www.the-numbers.com/movie/budgets/all/101',\n",
    "'https://www.the-numbers.com/movie/budgets/all/201',\n",
    "'https://www.the-numbers.com/movie/budgets/all/301',\n",
    "'https://www.the-numbers.com/movie/budgets/all/401',\n",
    "'https://www.the-numbers.com/movie/budgets/all/501',\n",
    "'https://www.the-numbers.com/movie/budgets/all/601',\n",
    "'https://www.the-numbers.com/movie/budgets/all/701',\n",
    "'https://www.the-numbers.com/movie/budgets/all/801',\n",
    "'https://www.the-numbers.com/movie/budgets/all/901',\n",
    "'https://www.the-numbers.com/movie/budgets/all/1001',\n",
    "'https://www.the-numbers.com/movie/budgets/all/1101',\n",
    "'https://www.the-numbers.com/movie/budgets/all/1201',\n",
    "'https://www.the-numbers.com/movie/budgets/all/1301',\n",
    "'https://www.the-numbers.com/movie/budgets/all/1401',\n",
    "'https://www.the-numbers.com/movie/budgets/all/1501',\n",
    "'https://www.the-numbers.com/movie/budgets/all/1601',\n",
    "'https://www.the-numbers.com/movie/budgets/all/1701',\n",
    "'https://www.the-numbers.com/movie/budgets/all/1801',\n",
    "'https://www.the-numbers.com/movie/budgets/all/1901',\n",
    "'https://www.the-numbers.com/movie/budgets/all/2001',\n",
    "'https://www.the-numbers.com/movie/budgets/all/2101',\n",
    "'https://www.the-numbers.com/movie/budgets/all/2201',\n",
    "'https://www.the-numbers.com/movie/budgets/all/2301',\n",
    "'https://www.the-numbers.com/movie/budgets/all/2401',\n",
    "'https://www.the-numbers.com/movie/budgets/all/2501',\n",
    "'https://www.the-numbers.com/movie/budgets/all/2601'\n",
    "\n",
    "]\n",
    "#Create an empty list to hold what df made from each URL page\n",
    "scraped_dfs = []\n",
    "\n",
    "for url in URLS:\n",
    "    response = get(url) #pull information in the dataframe\n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser') #parse text\n",
    "    main_table = html_soup.find_all('table')#grabs all table objects found on page\n",
    "    table = str(main_table[0]) #select the table we actually want happened to be the 1st table object on tha pge\n",
    "    dfs = pd.read_html(table)#createa df from everthing in table\n",
    "    df = dfs[0]#assign series as final df from page \n",
    "    df = df.drop([0]) #dropped column titles that happen on every page\n",
    "    df = df.drop([0], axis = 1) #drop rank becaue redundant with index\n",
    "    scraped_dfs.append(df) #add df to a list to concat after the loop\n",
    "\n",
    "BudgetDF = pd.concat(scraped_dfs)#concat all the df in the scraped_dfs list\n",
    "#recover the column names\n",
    "BudgetDF.columns = ['Release Date','Movie','Budget','Domestic Gross','Worldwide Gross']\n",
    "#save original data to csv file\n",
    "BudgetDF.to_csv('Budget_DF_original.csv') #commented out becuause already created\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in Data from Kaggle IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser error occurred.  Skipping bad lines...\n"
     ]
    }
   ],
   "source": [
    "#data set had rows that were not consistent with df shape, so we had to skip rows with errors\n",
    "try:\n",
    "   #do something\n",
    "   df = pd.read_csv('imdb_original.csv')\n",
    "except:\n",
    "   #handle your exception e\n",
    "   print('Parser error occurred.  Skipping bad lines...')\n",
    "   df = pd.read_csv('imdb_original.csv', error_bad_lines=False, warn_bad_lines=False)\n",
    "\n",
    "IMDB_Kag_DF = df\n",
    "\n",
    "#resave file so we do not encounter errors again\n",
    "IMDB_Kag_DF.to_csv('IMDB_Kag_DF_original.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
